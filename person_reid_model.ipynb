{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. **Video Preprocessing:**\n",
        "\n",
        "    Use a video processing library like OpenCV to read the video frames, split the video into individual frames, and save them as images. You can specify a frame rate to control how many frames are extracted from the video."
      ],
      "metadata": {
        "id": "c9ooW_1PUQDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "video_path = '/content/cctv_footage.mp4'\n",
        "output_folder = '/content/frame_folder/'\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frame_rate = 15  # Adjust as needed\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_count % frame_rate == 0:\n",
        "        frame_filename = f\"{output_folder}/frame_{frame_count}.jpg\"\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "qWdJJEu7T-na"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ut2yWgYjT-1J",
        "outputId": "4d78b5d6-7163-4106-f316-8d21d7244b21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/frame_folder/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Data Loading:**\n",
        "\n",
        "    Use PyTorch's DataLoader to load and augment the data. You can use libraries like torchvision for common data augmentations."
      ],
      "metadata": {
        "id": "4o4Kz8HyVaGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class FrameDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.frame_list = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.frame_list[idx])\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# Update the path to your frame directory\n",
        "frame_folder = '/content/frame_folder'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 64)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = FrameDataset(root_dir=frame_folder, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "ZCe5eRG9VZUS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfMts-w1T-2-",
        "outputId": "9185bdbc-97c3-4cd5-9c9e-db293c4178ab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.FrameDataset at 0x7d77d2a02860>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Model Architecture:**\n",
        "\n",
        "    Design the Siamese network or triplet network architecture. Here's an example of a Siamese network using ResNet as the base:"
      ],
      "metadata": {
        "id": "LQV4TWmycZz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#modify\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.base_model = nn.Sequential(*list(base_model.children())[:-1])\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "        )\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x  # Return a single tensor for the embedding\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        return output1, output2  # Return both output1 and output2\n"
      ],
      "metadata": {
        "id": "xAbRvaQ4jBG0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Loss Function:**"
      ],
      "metadata": {
        "id": "vewtbHM_cm0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = torch.norm(anchor - positive, p=2, dim=1)\n",
        "        distance_negative = torch.norm(anchor - negative, p=2, dim=1)\n",
        "        loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "1FYXLAglcihF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, frame_dataset):\n",
        "        self.frame_dataset = frame_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor = self.frame_dataset[idx]\n",
        "\n",
        "        # Find a positive sample (another image of the same person)\n",
        "        same_person_samples = [i for i in range(len(self.frame_dataset)) if i != idx]\n",
        "        positive_idx = random.choice(same_person_samples)\n",
        "        positive = self.frame_dataset[positive_idx]\n",
        "\n",
        "        # Find a negative sample (an image of a different person)\n",
        "        different_person_samples = [i for i in range(len(self.frame_dataset))]\n",
        "        different_person_samples.remove(idx)  # Remove anchor index\n",
        "        negative_idx = random.choice(different_person_samples)\n",
        "        negative = self.frame_dataset[negative_idx]\n",
        "\n",
        "        return anchor, positive, negative\n",
        "\n",
        "# Update the path to your frame directory\n",
        "frame_folder = '/content/frame_folder'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 64)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "frame_dataset = FrameDataset(root_dir=frame_folder, transform=transform)\n",
        "triplet_dataset = TripletDataset(frame_dataset)\n",
        "dataloader = DataLoader(triplet_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "f4r36JPNc_3T"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Training:**\n",
        "\n",
        "    Implement the training loop. You will need to define an optimizer and iterate through your dataset."
      ],
      "metadata": {
        "id": "ytujpnkDcvfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "TZk6yvmre_nn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, input1, input2):\n",
        "    output1 = self.forward_one(input1)\n",
        "    output2 = self.forward_one(input2)\n",
        "    return output1, output2"
      ],
      "metadata": {
        "id": "gUopEdX3iQz2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = SiameseNetwork(models.resnet18(pretrained=True))\n",
        "triplet_loss = TripletLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        anchor, positive, negative = batch\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output1, output2 = model(anchor, positive)\n",
        "        anchor_embedding = model(anchor, anchor)  # Compute the anchor's embedding separately\n",
        "        loss = triplet_loss(output1, output2, anchor_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "G3wkRj5EctBU",
        "outputId": "d10b6116-6ba5-488f-ef19-a24f9dfd46d2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-e6f09d2ef33a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0manchor_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the anchor's embedding separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-6dfd19329623>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, anchor, positive, negative)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdistance_positive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdistance_negative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_positive\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdistance_negative\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Tensor' and 'tuple'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Evaluation:**\n",
        "\n",
        "    After training, you can perform person re-identification on the video frames using the trained model. Here's an example of how you can extract features and compare them:"
      ],
      "metadata": {
        "id": "v5c86E6_jgDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "query_frame = cv2.imread('path_to_query_frame.jpg')\n",
        "gallery_frames = [cv2.imread(f) for f in gallery_frame_filenames]\n",
        "\n",
        "# Preprocess frames and convert to PyTorch tensors\n",
        "transform = transforms.Compose([transforms.Resize((128, 64)), transforms.ToTensor()])\n",
        "query_tensor = transform(query_frame).unsqueeze(0)  # Add batch dimension\n",
        "gallery_tensors = [transform(frame) for frame in gallery_frames]\n",
        "\n",
        "# Extract features\n",
        "query_features = model.forward_one(query_tensor.to(device))\n",
        "gallery_features = [model.forward_one(frame.to(device)) for frame in gallery_tensors]\n",
        "\n",
        "\n",
        "similarity_scores = [torch.nn.functional.cosine_similarity(query_features, gf) for gf in gallery_features]\n",
        "\n",
        "# Identify the index of the best match\n",
        "best_match_index = torch.argmax(similarity_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "g-YIWElSd7ew",
        "outputId": "9d99b66f-f0b6-4dc5-fae5-c4e35949b15c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-5342c740323a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquery_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path_to_query_frame.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgallery_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgallery_frame_filenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Preprocess frames and convert to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gallery_frame_filenames' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oUnjFcxTjrcE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}