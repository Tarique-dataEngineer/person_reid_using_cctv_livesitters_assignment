{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh83nB7yr40G"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Open the video file\n",
        "video_capture = cv2.VideoCapture('cctv_footage.mp4')\n",
        "\n",
        "# Create a directory to save the frames as image files\n",
        "output_directory = 'frames_output'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "frame_count = 0\n",
        "\n",
        "# Initialize the object detection model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Create a function to preprocess frames (similar to what you used for detection)\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = frame.astype(np.float32)\n",
        "    frame = frame / 127.5 - 1.0\n",
        "    return frame\n",
        "\n",
        "# Initialize an OrderedDict to store tracked objects and their corresponding trackers\n",
        "\n",
        "tracked_objects = OrderedDict()\n",
        "\n",
        "# Create an object tracker (e.g., KLT Tracker) for tracking persons\n",
        "tracker = cv2.TrackerKLT_create()\n",
        "\n",
        "while True:\n",
        "    ret, frame = video_capture.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # Save the frame as an image file in the output directory\n",
        "    frame_filename = os.path.join(output_directory, f'frame_{frame_count:04d}.jpg')\n",
        "    cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "    # Preprocess the frame\n",
        "    preprocessed_frame = preprocess_frame(frame)\n",
        "\n",
        "    # Perform object detection with the model\n",
        "    image_tensor = F.to_tensor(Image.fromarray(frame)).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_tensor)\n",
        "\n",
        "    # Extract \"person\" class bounding boxes\n",
        "    labels = prediction[0]['labels']\n",
        "    boxes = prediction[0]['boxes']\n",
        "    person_boxes = [boxes[i] for i in range(len(boxes)) if labels[i] == 1]\n",
        "\n",
        "    # Update the trackers and track persons\n",
        "    for object_id, (x, y, w, h) in list(tracked_objects.items()):\n",
        "        success, new_box = tracker.update(preprocessed_frame)\n",
        "        if success:\n",
        "            tracked_objects[object_id] = new_box\n",
        "        else:\n",
        "            # If tracking fails, remove the object from tracking\n",
        "            del tracked_objects[object_id]\n",
        "\n",
        "    # Add new detections to the tracked objects\n",
        "    for detection in person_boxes:\n",
        "        x, y, w, h = map(int, detection)\n",
        "        tracked_objects[len(tracked_objects)] = (x, y, w, h)\n",
        "        # Initialize a tracker for the newly detected person\n",
        "        tracker.init(preprocessed_frame, (x, y, w, h))\n",
        "\n",
        "    # Draw bounding boxes for tracked persons\n",
        "    for object_id, (x, y, w, h) in list(tracked_objects.items()):\n",
        "        x, y, w, h = map(int, (x, y, w, h))\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame with tracked persons\n",
        "    cv2.imshow('Object Tracking', frame)\n",
        "\n",
        "    # Press 'q' to exit the video\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KHBo0sVbBPAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "koaWQajQBOyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "j_Y7c_oFr_M-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the video file\n",
        "video_capture = cv2.VideoCapture('cctv_footage.mp4')\n",
        "\n",
        "# Create a directory to save the frames as image files\n",
        "output_directory = 'frames_output'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "frame_count = 0\n"
      ],
      "metadata": {
        "id": "iD_np7CHsAqx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the object detection model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Create a function to preprocess frames (similar to what you used for detection)\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = frame.astype(np.float32)\n",
        "    frame = frame / 127.5 - 1.0\n",
        "    return frame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUoR18VTsc_Z",
        "outputId": "318012e1-9ef0-4052-b847-5274fd6faea1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:00<00:00, 180MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "# Initialize an OrderedDict to store tracked objects and their corresponding trackers\n",
        "tracked_objects = OrderedDict()\n"
      ],
      "metadata": {
        "id": "lX1zuDTTsnMu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an object tracker (e.g., KCF Tracker) for tracking persons\n",
        "tracker = cv2.TrackerKCF_create()\n",
        "\n",
        "while True:\n",
        "    ret, frame = video_capture.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # Save the frame as an image file in the output directory\n",
        "    frame_filename = os.path.join(output_directory, f'frame_{frame_count:04d}.jpg')\n",
        "    cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "    # Preprocess the frame\n",
        "    preprocessed_frame = preprocess_frame(frame)\n",
        "\n",
        "    # Update the trackers and track persons\n",
        "    for object_id, (x, y, w, h) in list(tracked_objects.items()):\n",
        "        success, new_box = tracker.update(preprocessed_frame)\n",
        "        if success:\n",
        "            tracked_objects[object_id] = new_box\n",
        "        else:\n",
        "            # If tracking fails, remove the object from tracking\n",
        "            del tracked_objects[object_id]\n",
        "\n",
        "    # Perform object detection with the model\n",
        "    with torch.no_grad():  # Corrected syntax here\n",
        "        image_tensor = F.to_tensor(Image.fromarray(frame)).unsqueeze(0)\n",
        "        prediction = model(image_tensor)\n",
        "\n",
        "    # Extract \"person\" class bounding boxes\n",
        "    labels = prediction[0]['labels']\n",
        "    boxes = prediction[0]['boxes']\n",
        "    person_boxes = [boxes[i] for i in range(len(boxes)) if labels[i] == 1]\n",
        "\n",
        "    # Add new detections to the tracked objects and initialize trackers\n",
        "    for detection in person_boxes:\n",
        "        x, y, w, h = map(int, detection)\n",
        "        tracked_objects[len(tracked_objects)] = (x, y, w, h)\n",
        "\n",
        "        # Initialize a tracker for the newly detected person\n",
        "        roi = (x, y, w, h)\n",
        "        tracker.init(preprocessed_frame, roi)\n",
        "\n",
        "    # Draw bounding boxes for tracked persons\n",
        "    for object_id, (x, y, w, h) in list(tracked_objects.items()):\n",
        "        x, y, w, h = map(int, (x, y, w, h))\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame with tracked persons\n",
        "    cv2.imshow('Object Tracking', frame)\n",
        "\n",
        "    # Press 'q' to exit the video\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "MnFULRomwGS0",
        "outputId": "9b90e50f-0b04-417c-c07c-c37310d07e7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-126c0b68a2b5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Update the trackers and track persons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mobject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracked_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtracked_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv_contrib/modules/tracking/src/trackerKCF.cpp:588: error: (-215:Assertion failed) compressed_sz<=src.channels() in function 'updateProjectionMatrix'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yJ6HGakP4CrC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}